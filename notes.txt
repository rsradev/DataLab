Spark   -> PS + U + NU
Delta   -> Doc + YT
Airflow -> U 
DevOps  -> BtoA, Go, Micro(U)
Kafka   -> YT
Python  -> Algo

Spark:
master/driver:
spark-class org.apache.spark.deploy.master.Master --host ProBook


worker/executor:
spark-class org.apache.spark.deploy.worker.Worker spark://ProBook:7077 --host ProBook

spark-submit --master spark://ProBook:7077 --name SparkSumitApp
--total-executors-cores 4 --executor-memory 2g --executor-cores 2 "SCRIPT LOCATION"


RDD:
    -> Native spark objects
    -> In memory 
    -> Split into partitions 
    -> Read only
    -> Resilient(autorecover)
   



kafka:

zookeeper:
$KAFKA_HOME/bin/zookeeper-server-start.sh $KAFKA_HOME/config/zookeeper.properties

start kafk:
$KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.properties 

producer:
$KAFKA_HOME/bin/kafka-console-producer.sh --topic sample-topic --broker-list localhost:9092

consumer:
$KAFKA_HOME/bin/kafka-console-consumer.sh --topic sample-topic --from-beginning --bootstrap-server localhost:9092
